{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph ConvNet for cosmology: part of sphere classification\n",
    "\n",
    "[Nathanaël Perraudin](http://perraudin.info), [Michaël Defferrard](http://deff.ch), Tomasz Kacprzak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "import healpy as hp\n",
    "\n",
    "from scnn import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (17, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Load and visualize spherical data \n",
    "\n",
    "Load two maps with the same PSD and different high order statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1 = hp.read_map('data/same_psd/kappa_omega_m_0p3.fits')\n",
    "img2 = hp.read_map('data/same_psd/kappa_omega_m_0p26.fits')\n",
    "img1 = hp.reorder(img1, r2n=True)\n",
    "img2 = hp.reorder(img2, r2n=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downsample the maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nside = 1024\n",
    "img1 = hp.ud_grade(img1, nside_out=Nside, order_in='NESTED')\n",
    "img2 = hp.ud_grade(img2, nside_out=Nside, order_in='NESTED')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the two maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmin = min(np.min(img1), np.min(img2))\n",
    "cmax = max(np.max(img1), np.max(img2))\n",
    "hp.mollview(img1, title='Map 1, omega_m=0.31, pk_norm=0.82, h=0.7', nest=True, min=cmin, max=cmax)\n",
    "hp.mollview(img2, title='Map 2, omega_m=0.26, sigma_8=0.91, h=0.7', nest=True, min=cmin, max=cmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us cut the sphere into 192 smaller subparts. We display 16 of them bellow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order = 4\n",
    "\n",
    "index = np.arange(hp.nside2npix(order)) + 2\n",
    "mask = np.zeros_like(index, dtype=np.bool)\n",
    "mask[:order**2] = 1\n",
    "index *= mask\n",
    "hp.mollview(index, title='Some sphere subparts', nest=True)\n",
    "\n",
    "marker = np.zeros(hp.nside2npix(order))\n",
    "marker[0] = 1\n",
    "hp.mollview(marker, title='Selected indexes', nest=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Data preparation\n",
    "\n",
    "### 2.1 Samples creation\n",
    "\n",
    "We here create samples by dividing the two complete spheres in patches (based on healpix sampling)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hp_split(img, order, nest=True):\n",
    "    \"\"\"\n",
    "    Split the data of different part of the sphere. \n",
    "    Return the splitted data and some possible index on the sphere.\n",
    "    \"\"\"\n",
    "    npix = len(img)\n",
    "    nside = hp.npix2nside(npix)\n",
    "    if hp.nside2order(nside) < order:\n",
    "        raise ValueError('Order not compatible with data.')\n",
    "    if not nest:\n",
    "        raise NotImplementedError('Implement the change of coordidinate.')\n",
    "    nsample = 12 * order**2\n",
    "    return img.reshape([nsample, npix//nsample]), np.arange(npix//nsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dict()\n",
    "data['class1'], _ = hp_split(img1, order=4)\n",
    "data['class2'], index = hp_split(img2, order=4)\n",
    "\n",
    "print('The data is of shape {}'.format(data['class1'].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at one data sample on the entire sphere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# npix = hp.nside2npix(nside)\n",
    "# mask = np.ones([npix])>0\n",
    "# mask[index] = False\n",
    "# hp.ma()\n",
    "\n",
    "img = img1.copy()\n",
    "img[data['class1'].shape[1]:] = hp.UNSEEN\n",
    "img = hp.ma(img)\n",
    "\n",
    "projected_map = hp.mollview(img, nest=True, return_projected_map=True, xsize=1600)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(projected_map[380:520, 530:670]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Normalization and train / test split \n",
    "\n",
    "Let us split the data into training and testing sets. The raw data is stored into `x_raw` and the histograms into `x_trans`. As a transformation, we cannot use the power spectrum density. Hence we do an histogram of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def histogram(x, cmin, cmax, bins=100):\n",
    "    if x.ndim == 1:\n",
    "        y, _ = np.histogram(x, bins=bins, range=[cmin, cmax])\n",
    "        return y.astype(float)\n",
    "    else:\n",
    "        y = np.empty((len(x), bins), float)\n",
    "        for i in range(len(x)):\n",
    "            y[i], _ = np.histogram(x[i], bins=bins, range=[cmin, cmax])\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize and transform the data, i.e. extract features.\n",
    "x_raw = np.vstack((data['class1'], data['class2']))\n",
    "x_raw = x_raw / np.mean(x_raw**2) # Apply some normalization (We do not want to affect the mean)\n",
    "cmin = np.min(x_raw)\n",
    "cmax = np.max(x_raw)\n",
    "x_hist = histogram(x_raw, cmin, cmax)\n",
    "x_trans = preprocessing.scale(x_hist)\n",
    "\n",
    "# Create the label vector.\n",
    "labels = np.zeros([x_raw.shape[0]], dtype=int)\n",
    "labels[len(data['class1']):] = 1\n",
    "\n",
    "# Random train / test split.\n",
    "ntrain = 300\n",
    "ret = train_test_split(x_raw, x_trans, labels, test_size=len(x_raw)-ntrain, shuffle=True)\n",
    "x_raw_train, x_raw_test, x_trans_train, x_trans_test, labels_train, labels_test = ret\n",
    "\n",
    "print('Class 1 VS class 2')\n",
    "print('  Training set: {} / {}'.format(np.sum(labels_train==0), np.sum(labels_train==1)))\n",
    "print('  Test set: {} / {}'.format(np.sum(labels_test==0), np.sum(labels_test==1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Histogram features visualization\n",
    "\n",
    "Let us first plot the mean and then each feature individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2)\n",
    "\n",
    "axes[0].plot(np.mean(x_hist[labels==0], axis=0), label='class 1')\n",
    "axes[0].plot(np.mean(x_hist[labels==1], axis=0), label='class 2')\n",
    "axes[0].legend()\n",
    "axes[0].set_title('Mean histogram accross each class')\n",
    "\n",
    "axes[1].plot(x_hist[labels==0].T, 'b')\n",
    "axes[1].plot(x_hist[labels==1].T, 'r')\n",
    "axes[1].set_title('Histograms of individual samples');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3  Classification using SVM\n",
    "\n",
    "Let us test classify our data using an SVM classifier.\n",
    "\n",
    "While running an SVM classifier on the data will fail because of their dimensionality, we observe that we can correctly classify our dataset using the histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_error(model, x, labels, name):\n",
    "    pred = model.predict(x)\n",
    "    error = sum(np.abs(pred - labels)) / len(labels)\n",
    "    print('{} error: {:.2%}'.format(name, error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(kernel='rbf')\n",
    "clf.fit(x_raw_train, labels_train)\n",
    "\n",
    "print_error(clf, x_raw_train, labels_train, 'Training')\n",
    "print_error(clf, x_raw_test, labels_test, 'Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(kernel='rbf')\n",
    "clf.fit(x_trans_train, labels_train) \n",
    "\n",
    "print_error(clf, x_trans_train, labels_train, 'Training')\n",
    "print_error(clf, x_trans_test, labels_test, 'Test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Classification using a spherical CNN\n",
    "\n",
    "Let us now classify our data using a spherical convolutional neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsides = [Nside, Nside, Nside//2, min(Nside//8, 128)]\n",
    "# nsides = [2048, 1024, 256, 64]\n",
    "# nsides = [128, 32, 16]\n",
    "\n",
    "nsample = 12 * order**2\n",
    "indexes = [np.arange(hp.nside2npix(nside)//nsample) for nside in nsides]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = 2 # number of class\n",
    "\n",
    "params = dict()\n",
    "params['dir_name']       = 'sphere_part'\n",
    "params['num_epochs']     = 10\n",
    "params['batch_size']     = 20\n",
    "params['eval_frequency'] = 10\n",
    "\n",
    "# Building blocks.\n",
    "params['brelu']          = 'b1lrelu' # Relu \n",
    "params['pool']           = 'apool1' # Average pooling\n",
    "\n",
    "# Architecture.\n",
    "params['nsides']         = nsides # Sizes of the laplacians are 12 * nsides**2.\n",
    "params['indexes']        = indexes # Sizes of the laplacians are 12 * nsides**2.\n",
    "params['F']              = [5, 20, 80, 10]  # Number of graph convolutional filters.\n",
    "params['K']              = [10, 10, 10, 10]  # Polynomial orders.\n",
    "params['batch_norm']     = [True, True, True, True]  # Batch norm.\n",
    "params['M']              = [100, C]  # Output dimensionality of fully connected layers.\n",
    "\n",
    "# Optimization.\n",
    "params['regularization'] = 2e-4\n",
    "params['dropout']        = 0.8\n",
    "params['learning_rate']  = 1e-3\n",
    "params['decay_rate']     = 0.95\n",
    "params['momentum']       = 0.9\n",
    "params['adam']           = True\n",
    "params['decay_steps']    = ntrain / params['batch_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.scnn(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy, loss, t_step = model.fit(x_raw_train, labels_train, x_raw_test, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_error(model, x_raw_train, labels_train, 'Training')\n",
    "print_error(model, x_raw_test, labels_test, 'Test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 5 Discussion\n",
    "\n",
    "Without subsampling\n",
    "I train the spherical CNN a few minutes on CPU and I obtain 96% validation accuracy. \n",
    "\n",
    "SVM is consistenly failling with the raw data but succeed with the histograms.\n",
    "\n",
    "Conclusion: the spherical CNN is able to discriminate over data with the same mean and same PSD using only 192th of the sphere.\n",
    "\n",
    "Effect of subsampling\n",
    " - N=512, errors on training/testing: 11.66%, 78.57% => complete fail\n",
    " - N=1024, errors on training/testing: 0%, 0-3% => partial success\n",
    " - N=2048, errors on training/testing: 0%, 3% => partial success\n",
    "\n",
    "Maybe this is also due to the fact that the training/validation sets are not the same for each run.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some other plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scnn import utils\n",
    "# nside_v = 32\n",
    "# nsample = 12 * (order**2)\n",
    "# ind = np.array(list(range(hp.nside2npix(nside_v)//nsample)))\n",
    "# G = utils.healpix_graph(nside=nside_v, nest=True, indexes=ind)\n",
    "\n",
    "# G.plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
