{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [DeepSphere]: a spherical convolutional neural network\n",
    "[DeepSphere]: https://github.com/SwissDataScienceCenter/DeepSphere\n",
    "\n",
    "[Nathanaël Perraudin](https://perraudin.info), [Michaël Defferrard](http://deff.ch), Tomasz Kacprzak, Raphael Sgier\n",
    "\n",
    "# Demo regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Run on CPU.\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "import healpy as hp\n",
    "import tensorflow as tf\n",
    "\n",
    "from deepsphere import models, experiment_helper, plot\n",
    "from deepsphere.data import LabeledDataset\n",
    "from deepsphere.utils import HiddenPrints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (17, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_NAME = 'regression'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Data loading\n",
    "\n",
    "Let us create a very simple syntetic dataset by simply filtering random noise with different Gaussian filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nside=32\n",
    "Nsamples = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arcmin2rad(x):\n",
    "    return x / 60 / 360 * 2 * np.pi\n",
    "def gaussian_smoothing(sig, sigma, nest=True):\n",
    "    if nest:\n",
    "        sig = hp.reorder(sig, n2r=True)\n",
    "    smooth = hp.sphtfunc.smoothing(sig, sigma=sigma)\n",
    "    if nest:\n",
    "        smooth = hp.reorder(smooth, r2n=True)\n",
    "    smooth/=np.linalg.norm(smooth)\n",
    "    return smooth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.randn(Nsamples, hp.nside2npix(Nside))\n",
    "t = np.random.rand(Nsamples,2)+0.5\n",
    "\n",
    "with HiddenPrints():\n",
    "    xs1 = np.array(list(map(gaussian_smoothing, x,t[:,0]/Nside)))    \n",
    "    xs2 = np.array(list(map(gaussian_smoothing, x,t[:,1]/Nside)))\n",
    "xs = np.stack((xs1,xs2), axis=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us plot a map of each class. It is not simple to visually catch the differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = plt.cm.RdBu_r\n",
    "cm.set_under('w')\n",
    "hp.mollview(xs[0,:,0], title='Smooth', nest=True, cmap=cm)\n",
    "hp.mollview(x[0], title='Original noise', nest=True, cmap=cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Data preparation\n",
    "\n",
    "Let us split the data into training and testing sets. The raw data is stored into `x_raw` and the power spectrum densities into `x_psd`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize and transform the data, i.e. extract features.\n",
    "x_raw = xs / np.mean(xs**2) # Apply some normalization (We do not want to affect the mean)\n",
    "\n",
    "# Create the label vector\n",
    "labels = t-0.5\n",
    "# Random train / test split\n",
    "ntrain = 800\n",
    "x_raw_train = x_raw[:ntrain]\n",
    "x_raw_test = x_raw[ntrain:]\n",
    "labels_train = labels[:ntrain]\n",
    "labels_test = labels[ntrain:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression using DeepSphere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = dict()\n",
    "params['dir_name'] = EXP_NAME\n",
    "\n",
    "# Types of layers.\n",
    "params['conv'] = 'chebyshev5'  # Graph convolution: chebyshev5 or monomials.\n",
    "params['pool'] = 'max'  # Pooling: max or average.\n",
    "params['activation'] = 'relu'  # Non-linearity: relu, elu, leaky_relu, softmax, tanh, etc.\n",
    "params['statistics'] = None  # Statistics (for invariance): None, mean, var, meanvar, hist.\n",
    "\n",
    "# Architecture.\n",
    "\n",
    "params['statistics'] = 'mean' # We build an invariance for the problem\n",
    "params['nsides'] = [Nside, Nside//2, Nside//4, Nside//8]  # Pooling: number of pixels per layer.\n",
    "params['F'] = [16, 32, 64]  # Graph convolutional layers: number of feature maps.\n",
    "params['M'] = [64, 2]  # Fully connected layers: output dimensionalities.\n",
    "params['loss'] = 'l2'\n",
    "params['input_channel'] = 2\n",
    "params['K'] = [5] * len(params['F'])  # Polynomial orders.\n",
    "params['batch_norm'] = [True] * len(params['F'])  # Batch normalization.\n",
    "\n",
    "# Regularization.\n",
    "params['regularization'] = 0  # Amount of L2 regularization over the weights (will be divided by the number of weights).\n",
    "params['dropout'] = 1  # Percentage of neurons to keep.\n",
    "\n",
    "# Training.\n",
    "params['num_epochs'] = 10  # Number of passes through the training data.\n",
    "params['batch_size'] = 16  # Number of samples per training batch. Should be a power of 2 for greater speed.\n",
    "params['eval_frequency'] = 15  # Frequency of model evaluations during training (influence training time).\n",
    "params['scheduler'] = lambda step: 1e-4  # Constant learning rate.\n",
    "params['optimizer'] = lambda lr: tf.train.GradientDescentOptimizer(lr)\n",
    "#params['optimizer'] = lambda lr: tf.train.MomentumOptimizer(lr, momentum=0.5)\n",
    "# params['optimizer'] = lambda lr: tf.train.AdamOptimizer(lr, beta1=0.9, beta2=0.99, epsilon=1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.deepsphere(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup before running again.\n",
    "shutil.rmtree('summaries/{}/'.format(EXP_NAME), ignore_errors=True)\n",
    "shutil.rmtree('checkpoints/{}/'.format(EXP_NAME), ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = LabeledDataset(x_raw_train, labels_train)\n",
    "testing = LabeledDataset(x_raw_test, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_validation, loss_validation, loss_training, t_step = model.fit(training, testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.plot_loss(loss_training, loss_validation, t_step, params['eval_frequency'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = model.predict(x_raw_test)\n",
    "pred_train = model.predict(x_raw_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(np.abs(pred_test-labels_test))/np.std(labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(labels_train[:,0], pred_train[:,0], 'o', label='Train')\n",
    "plt.plot(labels_test[:,0], pred_test[:,0], 'o', label='Test')\n",
    "plt.plot([0,1],[0,1])\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(labels_train[:,1], pred_train[:,1], 'o', label='Train')\n",
    "plt.plot(labels_test[:,1], pred_test[:,1], 'o', label='Test')\n",
    "plt.plot([0,1],[0,1])\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
