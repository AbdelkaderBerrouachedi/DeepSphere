{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph ConvNet for cosmology: part of sphere classification (whole data, smoothing + noise)\n",
    "\n",
    "[Nathanaël Perraudin](http://perraudin.info), [Michaël Defferrard](http://deff.ch), Tomasz Kacprzak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from tqdm import tqdm_notebook\n",
    "import healpy as hp\n",
    "import pygsp\n",
    "\n",
    "from scnn import models, utils\n",
    "from scnn.data import LabeledDatasetWithNoise, LabeledDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (17, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nside = 1024\n",
    "order = 2  # 1,2,4,8 correspond to 12,48,192,768 parts of the sphere.\n",
    "sigma_noise = 10\n",
    "EXP_NAME = '40sim_{}sides_1arcmin_{}noise_{}order'.format(Nside, sigma_noise, order)\n",
    "data_path = 'data/same_psd/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Load spherical data \n",
    "\n",
    "Load the different maps and prepare some dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1 = []\n",
    "ds2 = []\n",
    "\n",
    "for filename in tqdm_notebook(os.listdir(data_path)):\n",
    "    \n",
    "    if not filename.endswith('fits'):\n",
    "        continue\n",
    "        \n",
    "    filepath = os.path.join(data_path, filename)\n",
    "    img = hp.read_map(filepath, verbose=False)\n",
    "    img = hp.reorder(img, r2n=True)\n",
    "    img = hp.ud_grade(img, nside_out=Nside, order_in='NESTED')\n",
    "    \n",
    "    if '0p26' in filename:\n",
    "        ds1.append(img)\n",
    "    elif '0p31' in filename:\n",
    "        ds2.append(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Data preparation\n",
    "\n",
    "Here we apply the preprocessing steps.\n",
    "1. Smooth with a radius of 1 arcmin.\n",
    "2. Cut the sphere into samples.\n",
    "3. Add noise.\n",
    "4. Split the data in training and test sets.\n",
    "\n",
    "### 2.1 Data smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arcmin2rad(x):\n",
    "    return x / 60 / 360 * 2 * np.pi\n",
    "\n",
    "ds1 = [hp.sphtfunc.smoothing(el, sigma=arcmin2rad(1)) for el in ds1]\n",
    "ds1 = [hp.sphtfunc.smoothing(el, sigma=arcmin2rad(1)) for el in ds1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Samples creation\n",
    "\n",
    "We here create samples by dividing the two complete spheres in patches (based on healpix sampling)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dict()\n",
    "data['class1'] = np.vstack([utils.hp_split(el, order=order) for el in ds1])\n",
    "data['class2'] = np.vstack([utils.hp_split(el, order=order) for el in ds2])\n",
    "del ds1\n",
    "del ds2\n",
    "\n",
    "print('The data is of shape {}'.format(data['class1'].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Normalization and train / test split \n",
    "\n",
    "Let us split the data into training and testing sets. The raw data is stored into `x_raw` and the histograms into `x_trans`. As a transformation, we cannot use the power spectrum density. Hence we do an histogram of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize and transform the data, i.e. extract features.\n",
    "x_raw = np.vstack((data['class1'], data['class2']))\n",
    "x_raw = x_raw / np.mean(abs(x_raw)) # Apply some normalization (We do not want to affect the mean)\n",
    "rs = np.random.RandomState(0)\n",
    "x_noise = x_raw + sigma_noise*rs.randn(*x_raw.shape)\n",
    "cmin = np.min(x_raw)\n",
    "cmax = np.max(x_raw)\n",
    "x_hist = utils.histogram(x_noise, cmin, cmax)\n",
    "x_trans = preprocessing.scale(x_hist)\n",
    "\n",
    "# Create the label vector.\n",
    "labels = np.zeros([x_raw.shape[0]], dtype=int)\n",
    "labels[len(data['class1']):] = 1\n",
    "\n",
    "# Random train / test split.\n",
    "ntrain = 1500\n",
    "ret = train_test_split(x_raw, x_trans, x_noise, labels, test_size=len(x_raw)-ntrain, shuffle=True, random_state=0)\n",
    "x_raw_train, x_raw_test, x_trans_train, x_trans_test, x_noise_train, x_noise_test, labels_train, labels_test = ret\n",
    "\n",
    "print('Class 1 VS class 2')\n",
    "print('  Training set: {} / {}'.format(np.sum(labels_train==0), np.sum(labels_train==1)))\n",
    "print('  Test set: {} / {}'.format(np.sum(labels_test==0), np.sum(labels_test==1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3  Classification using SVM\n",
    "\n",
    "Let us test classify our data using an SVM classifier.\n",
    "\n",
    "While running an SVM classifier on the data will fail because of their dimensionality, we observe that we can correctly classify our dataset using the histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(kernel='rbf')\n",
    "clf.fit(np.vstack([x_trans_train, utils.histogram(x_raw_train, cmin, cmax)]),\n",
    "        np.concatenate([labels_train, labels_train]))\n",
    "# clf.fit(x_trans_train, labels_train) \n",
    "\n",
    "utils.print_error(clf, x_trans_train, labels_train, 'Training')\n",
    "utils.print_error(clf, x_trans_test, labels_test, 'Test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Histogram features visualization\n",
    "\n",
    "Let us first plot the mean and then each feature individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(cmin, cmax, num=x_hist.shape[1])\n",
    "fig, axes = plt.subplots(1, 2)\n",
    "np.linspace(cmin, cmax, num=x_hist.shape[0])\n",
    "axes[0].plot(x, np.mean(x_hist[labels==0], axis=0), label='class 1')\n",
    "axes[0].plot(x, np.mean(x_hist[labels==1], axis=0), label='class 2')\n",
    "axes[0].legend()\n",
    "axes[0].set_title('Mean histogram accross each class')\n",
    "\n",
    "axes[1].plot(x, x_hist[labels==0].T, 'b')\n",
    "axes[1].plot(x, x_hist[labels==1].T, 'r')\n",
    "axes[1].set_title('Histograms of individual samples');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Classification using a spherical CNN\n",
    "\n",
    "Let us now classify our data using a spherical convolutional neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = LabeledDatasetWithNoise(x_raw_train, labels_train, start_level=0, end_level=sigma_noise, nit=200)\n",
    "testing = LabeledDataset(x_noise_test, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nsides = [Nside, Nside//2, Nside//4, min(Nside//8, 128)]\n",
    "nsides = [Nside, Nside//2, Nside//4, Nside//8, min(Nside//16, 128)]\n",
    "\n",
    "# nsides = [2048, 1024, 256, 64]\n",
    "# nsides = [128, 32, 16]\n",
    "print('#sides: {}'.format(nsides))\n",
    "\n",
    "indexes = utils.nside2indexes(nsides, order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup before running again.\n",
    "shutil.rmtree('summaries/{}/'.format(EXP_NAME), ignore_errors=True)\n",
    "shutil.rmtree('checkpoints/{}/'.format(EXP_NAME), ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = 2 # number of class\n",
    "\n",
    "params = dict()\n",
    "params['dir_name']       = EXP_NAME\n",
    "params['num_epochs']     = 10\n",
    "params['batch_size']     = 10\n",
    "params['eval_frequency'] = 10\n",
    "\n",
    "# Building blocks.\n",
    "params['brelu']          = 'b1relu'  # Activation.\n",
    "params['pool']           = 'mpool1'  # Pooling.\n",
    "\n",
    "# Architecture.\n",
    "params['nsides']         = nsides  # Sizes of the laplacians are 12 * nsides**2.\n",
    "params['indexes']        = indexes  # Sizes of the laplacians are 12 * nsides**2.\n",
    "params['F']              = [10, 40, 160, 40, 10]  # Number of graph convolutional filters.\n",
    "params['K']              = [10, 10, 10, 10, 10]  # Polynomial orders.\n",
    "params['batch_norm']     = [True, True, True, True, True]  # Batch norm\n",
    "params['M']              = [100, C]  # Output dimensionality of fully connected layers.\n",
    "\n",
    "# Optimization.\n",
    "params['regularization'] = 1e-4\n",
    "params['dropout']        = 0.5\n",
    "params['learning_rate']  = 1e-4\n",
    "params['decay_rate']     = 0.9\n",
    "params['momentum']       = 0.9\n",
    "params['adam']           = True\n",
    "params['decay_steps']    = ntrain / params['batch_size']\n",
    "\n",
    "model = models.scnn(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy, loss, t_step = model.fit(training, testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.print_error(model, x_noise_train, labels_train, 'Training')\n",
    "utils.print_error(model, x_noise_test, labels_test, 'Test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 5 Discussion\n",
    "\n",
    "Here are a few results with the seed fixed to 0 for reproducibility.\n",
    "\n",
    "|Noise |Histogram | SCNN    | Epochs |\n",
    "|:-:   |:-:       |:-:      |:-:     |\n",
    "| 0    | 0%       | 0%      | 1      |\n",
    "| 5    | 0%       | 0%      | 5      |\n",
    "| 10   | 1.43%    | 0.95%   | 10     | Peak 0.48%\n",
    "| 15   | 21.67%   | 15%     | 10     |\n",
    "| 20   | 40.71%   | 23.57%  | 20     |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 Filters visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'results/filters/{}/'.format(EXP_NAME)\n",
    "try:\n",
    "    os.mkdir(folder)\n",
    "except FileExistsError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = 1\n",
    "\n",
    "K, Fout = params['K'][layer-1], params['F'][layer-1]\n",
    "trained_weights = model.get_var('conv{}/weights'.format(layer))  # Fin*K x Fout\n",
    "trained_weights = trained_weights.reshape((-1, K, Fout))\n",
    "if layer >= 2:\n",
    "    Fin = params['F'][layer-2]\n",
    "    assert trained_weights.shape == (Fin, K, Fout)\n",
    "Fin, K, Fout = trained_weights.shape\n",
    "\n",
    "# Fin x K x Fout => K x Fout x Fin\n",
    "trained_weights = trained_weights.transpose([1, 2, 0])\n",
    "trained_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(trained_weights.reshape((K, Fin*Fout)), '.')\n",
    "ax.set_title('Learned Chebyshev coefficients')\n",
    "fig.savefig('{}/layer{}_coefficients.png'.format(folder, layer), dpi=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check.\n",
    "# trained_weights = np.identity(5)\n",
    "\n",
    "nside = params['nsides'][layer-1]\n",
    "G = utils.healpix_graph(nside=nside)\n",
    "G.estimate_lmax()\n",
    "\n",
    "filters = pygsp.filters.Chebyshev(G, trained_weights)\n",
    "\n",
    "# Sanity check.\n",
    "# filters = pygsp.filters.Heat(G, tau=[5, 10, 20, 50])\n",
    "# filters = filters.approximate('Chebyshev', order=4)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "filters.plot(sum=False, ax=ax)\n",
    "fig.savefig('{}/layer{}_spectral.png'.format(folder, layer), dpi=100)\n",
    "\n",
    "fig = utils.plot_filters_section(filters)\n",
    "fig.savefig('{}/layer{}_section.png'.format(folder, layer), dpi=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
