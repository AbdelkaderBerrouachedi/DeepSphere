{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph ConvNet for cosmology: whole sphere classification\n",
    "\n",
    "[Nathanaël Perraudin](http://perraudin.info), [Michaël Defferrard](http://deff.ch), Tomasz Kacprzak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from scnn import experiment_helper\n",
    "from scnn import models, utils, plot\n",
    "from scnn.data import LabeledDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (17, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_NAME = 'whole_sphere'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Data loading\n",
    "\n",
    "The data consists of a toy dataset that is sufficiently small to have fun with. It is made of 200 maps of size `NSIDE=64` splitted into 2 classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('data/maps_downsampled_64.npz')\n",
    "assert(len(data['class1']) == len(data['class2']))\n",
    "nclass = len(data['class1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us plot a map of each class. It is not simple to visually catch the differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmin = min(np.min(data['class1']), np.min(data['class2']))\n",
    "cmax = max(np.max(data['class1']), np.max(data['class2']))\n",
    "cm = plt.cm.RdBu_r\n",
    "cm.set_under('w')\n",
    "hp.mollview(data['class1'][0], title='class 1', nest=True, cmap=cm, min=cmin, max=cmax)\n",
    "hp.mollview(data['class2'][0], title='class 2', nest=True, cmap=cm, min=cmin, max=cmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, those maps have different Power Spectral Densities PSD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_psd_class1 = np.empty((nclass, 192))\n",
    "sample_psd_class2 = np.empty((nclass, 192))\n",
    "\n",
    "for i in range(nclass):\n",
    "    sample_psd_class1[i] = experiment_helper.psd(data['class1'][i])\n",
    "    sample_psd_class2[i] = experiment_helper.psd(data['class2'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ell = np.arange(sample_psd_class1.shape[1])\n",
    "plot.plot_with_std(ell, sample_psd_class1*ell*(ell+1), label='class 1, Omega_matter=0.3, mean', color='b')\n",
    "plot.plot_with_std(ell,sample_psd_class2*ell*(ell+1), label='class 2, Omega_matter=0.5, mean', color='r')\n",
    "plt.legend(fontsize=16);\n",
    "plt.xlim([10, np.max(ell)])\n",
    "plt.ylim([1e-6, 1e-3])\n",
    "# plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.xlabel('$\\ell$: spherical harmonic index', fontsize=18)\n",
    "plt.ylabel('$C_\\ell \\cdot \\ell \\cdot (\\ell+1)$', fontsize=18)\n",
    "plt.title('Power Spectrum Density, 3-arcmin smoothing, noiseless, Nside=1024', fontsize=18);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Data preparation\n",
    "\n",
    "Let us split the data into training and testing sets. The raw data is stored into `x_raw` and the power spectrum densities into `x_psd`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize and transform the data, i.e. extract features.\n",
    "x_raw = np.vstack((data['class1'], data['class2']))\n",
    "x_raw = x_raw / np.mean(x_raw**2) # Apply some normalization (We do not want to affect the mean)\n",
    "x_psd = preprocessing.scale(np.vstack((sample_psd_class1, sample_psd_class2)))\n",
    "\n",
    "# Create the label vector\n",
    "labels = np.zeros([x_raw.shape[0]], dtype=int)\n",
    "labels[nclass:] = 1\n",
    "\n",
    "# Random train / test split\n",
    "ntrain = 150\n",
    "ret = train_test_split(x_raw, x_psd, labels, test_size=2*nclass-ntrain, shuffle=True)\n",
    "x_raw_train, x_raw_test, x_psd_train, x_psd_test, labels_train, labels_test = ret\n",
    "\n",
    "print('Class 1 VS class 2')\n",
    "print('  Training set: {} / {}'.format(np.sum(labels_train==0), np.sum(labels_train==1)))\n",
    "print('  Test set: {} / {}'.format(np.sum(labels_test==0), np.sum(labels_test==1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Classification using SVM\n",
    "\n",
    "As a baseline, let us classify our data using an SVM classifier.\n",
    "\n",
    "* An SVM based on the raw feature cannot discriminate the data because the dimensionality of the data is too large.\n",
    "* We however observe that the PSD features are linearly separable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(kernel='rbf')\n",
    "clf.fit(x_raw_train, labels_train) \n",
    "\n",
    "e_train = experiment_helper.model_error(clf, x_raw_train, labels_train)\n",
    "e_test = experiment_helper.model_error(clf, x_raw_test, labels_test)\n",
    "print('The training error is: {}%'.format(e_train*100))\n",
    "print('The testing error is: {}%'.format(e_test*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(kernel='linear')\n",
    "clf.fit(x_psd_train, labels_train) \n",
    "\n",
    "e_train = experiment_helper.model_error(clf, x_psd_train, labels_train)\n",
    "e_test = experiment_helper.model_error(clf, x_psd_test, labels_test)\n",
    "print('The training error is: {}%'.format(e_train*100))\n",
    "print('The testing error is: {}%'.format(e_test*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Classification using a spherical CNN\n",
    "\n",
    "Let us now classify our data using a spherical convolutional neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = dict()\n",
    "params['dir_name']       = EXP_NAME\n",
    "params['num_epochs']     = 5\n",
    "params['batch_size']     = 10\n",
    "params['eval_frequency'] = 10\n",
    "\n",
    "# Building blocks.\n",
    "params['brelu']          = 'b1relu'  # Activation.\n",
    "params['pool']           = 'apool1'  # Pooling.\n",
    "\n",
    "# Architecture.\n",
    "params['nsides']         = [64, 32, 16]  # Sizes of the laplacians are 12 * nsides**2.\n",
    "params['F']              = [5, 10, 10]  # Number of graph convolutional filters.\n",
    "params['K']              = [10, 10, 10]  # Polynomial orders.\n",
    "params['batch_norm']     = [True, True, True]  # Batch norm.\n",
    "params['M']              = [100, 2]  # Output dimensionality of fully connected layers.\n",
    "\n",
    "# Optimization.\n",
    "params['regularization'] = 1e-4\n",
    "params['dropout']        = 0.5 # 1 is no dropout\n",
    "params['learning_rate']  = 1e-3\n",
    "params['decay_rate']     = 0.98\n",
    "params['momentum']       = 0.9\n",
    "params['adam']           = True\n",
    "params['decay_steps']    = ntrain / params['batch_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.scnn(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = LabeledDataset(x_raw_train, labels_train)\n",
    "testing = LabeledDataset(x_raw_test, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy, loss, t_step = model.fit(training, testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_train = experiment_helper.model_error(model, x_raw_train, labels_train)\n",
    "e_test = experiment_helper.model_error(model, x_raw_test, labels_test)\n",
    "print('The training error is: {}%'.format(e_train*100))\n",
    "print('The testing error is: {}%'.format(e_test*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Filters visualization\n",
    "\n",
    "The package offers a few different visualizations for the learned filters. First we can simply look at the Chebyshef coefficients. This visualization is not very interpretable for human, but can help for debugging problems related to optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer=2\n",
    "model.plot_chebyshev_coeffs(layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe the Chebyshef polynomial, i.e the filters in the graph spectral domain. This visuallization can help to understand wich graph frequencies are picked by the filtering operation. It mostly interpretable by the people for the graph signal processing community."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.plot_filters_spectral(layer);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here comes one of the most human friendly representation of the filters. It consists the section of the filters \"projected\" on the sphere. Because of the irregularity of the healpix sampling, this representation of the filters may not look very smooth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams.update({'font.size': 16})\n",
    "model.plot_filters_section(layer, title='');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eventually, we can simply look at the filters on sphere. This representation clearly displays the sampling artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.plot_filters_gnomonic(layer, title='');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
