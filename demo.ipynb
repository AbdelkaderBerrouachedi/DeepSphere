{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph ConvNet for cosmology: whole sphere classification\n",
    "\n",
    "[Nathanaël Perraudin](http://perraudin.info), [Michaël Defferrard](http://deff.ch), Tomasz Kacprzak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "import healpy as hp\n",
    "\n",
    "from scnn import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (17, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Load some spherical data \n",
    "\n",
    "The data consists of ...\n",
    "\n",
    "The produced maps have been down-sampled from `NSIDE=1024` to `NSIDE=64` using the `ud_grade` function of the `healpy` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('data/maps_downsampled_64.npz')\n",
    "assert(len(data['class1']) == len(data['class2']))\n",
    "nclass = len(data['class1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us plot a map of each class. It is not easy to visually see a difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmin = min(np.min(data['class1']), np.min(data['class2']))\n",
    "cmax = max(np.max(data['class1']), np.max(data['class2']))\n",
    "hp.mollview(data['class1'][0], title='class 1, Omega_matter=0.7', nest=True,  cmap='jet', min=cmin, max=cmax)\n",
    "hp.mollview(data['class2'][0], title='class 2, Omega_matter=0.5', nest=True,  cmap='jet', min=cmin, max=cmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, those maps have different power spectral densities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def psd(x):\n",
    "    '''Spherical Power Spectral Densities'''\n",
    "    hatx = hp.map2alm(hp.reorder(x, n2r=True))\n",
    "    return hp.alm2cl(hatx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE = 0\n",
    "plt.semilogx(psd(data['class1'][SAMPLE]), label='class 1, Omega_matter=0.3, sample {}'.format(SAMPLE))\n",
    "plt.semilogx(psd(data['class2'][SAMPLE]), label='class 2, Omega_matter=0.5, sample {}'.format(SAMPLE))\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When averaging over all the samples, and hence getting closer to the true PSD estimate, the statistical difference becomes obvious."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_psd_class1 = np.empty((nclass, 192))\n",
    "sample_psd_class2 = np.empty((nclass, 192))\n",
    "\n",
    "for i in range(nclass):\n",
    "    sample_psd_class1[i] = psd(data['class1'][i])\n",
    "    sample_psd_class2[i] = psd(data['class2'][i])\n",
    "\n",
    "psd_class1 = np.mean(sample_psd_class1, axis=0)\n",
    "psd_class2 = np.mean(sample_psd_class2, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogx(psd_class1, label='class 1, Omega_matter=0.3, mean')\n",
    "plt.semilogx(psd_class2, label='class 2, Omega_matter=0.5, mean')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Data preparation\n",
    "\n",
    "Let us split the data into training and testing sets. The raw data is stored into `x_raw` and the power spectrum densities into `x_psd`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put all the data in a single matrix\n",
    "x_raw = np.vstack((data['class1'], data['class2']))\n",
    "x_raw = x_raw / np.mean(x_raw**2) # Apply some normalization (We do not want to affect the mean)\n",
    "x_psd = preprocessing.scale(np.vstack((sample_psd_class1, sample_psd_class2)))\n",
    "\n",
    "# Create the label vector\n",
    "labels = np.zeros([x_raw.shape[0]], dtype=int)\n",
    "labels[nclass:] = 1\n",
    "\n",
    "# Random train / test split\n",
    "ntrain = 150\n",
    "ret = train_test_split(x_raw, x_psd, labels, test_size=2*nclass-ntrain, shuffle=True)\n",
    "x_raw_train, x_raw_test, x_psd_train, x_psd_test, labels_train, labels_test = ret\n",
    "\n",
    "print('Class 1 VS class 2')\n",
    "print('  Training set: {} / {}'.format(np.sum(labels_train==0), np.sum(labels_train==1)))\n",
    "print('  Test set: {} / {}'.format(np.sum(labels_test==0), np.sum(labels_test==1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Classification using SVM\n",
    "\n",
    "As a baseline, let us classify our data using an SVM classifier.\n",
    "\n",
    "* An SVM based on the raw feature cannot discriminate the data because the dimensionality of the data is too large.\n",
    "* We however observe that the PSD features are linearly separable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_error(model, x, labels, name):\n",
    "    pred = model.predict(x)\n",
    "    error = sum(np.abs(pred - labels)) / len(labels)\n",
    "    print('{} error: {:.2%}'.format(name, error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(kernel='rbf')\n",
    "clf.fit(x_raw_train, labels_train) \n",
    "\n",
    "print_error(clf, x_raw_train, labels_train, 'Training')\n",
    "print_error(clf, x_raw_test, labels_test, 'Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(kernel='linear')\n",
    "clf.fit(x_psd_train, labels_train) \n",
    "\n",
    "print_error(clf, x_psd_train, labels_train, 'Training')\n",
    "print_error(clf, x_psd_test, labels_test, 'Test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Classification using a spherical CNN\n",
    "\n",
    "Let us now classify our data using a spherical convolutional neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = dict()\n",
    "params['dir_name']       = 'sphere_whole'\n",
    "params['num_epochs']     = 5\n",
    "params['batch_size']     = 10\n",
    "params['eval_frequency'] = 10\n",
    "\n",
    "# Building blocks.\n",
    "params['brelu']          = 'b1relu' # Relu\n",
    "params['pool']           = 'apool1' # Average pooling\n",
    "\n",
    "# Architecture.\n",
    "params['nsides']         = [64, 32, 16] # Sizes of the laplacians are nsides * nisides * 12\n",
    "params['F']              = [5, 10, 10]  # Number of graph convolutional filters.\n",
    "params['K']              = [10, 10, 10]  # Polynomial orders.\n",
    "params['batch_norm']     = [True, True, True]  # Batch norm\n",
    "params['M']              = [100, 2]  # Output dimensionality of fully connected layers.\n",
    "\n",
    "# Optimization.\n",
    "params['regularization'] = 1e-4\n",
    "params['dropout']        = 0.8\n",
    "params['learning_rate']  = 1e-3\n",
    "params['decay_rate']     = 0.95\n",
    "params['momentum']       = 0.9\n",
    "params['adam']           = True\n",
    "params['decay_steps']    = ntrain / params['batch_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.scnn(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy, loss, t_step = model.fit(x_raw_train, labels_train, x_raw_test, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_error(model, x_raw_train, labels_train, 'Training')\n",
    "print_error(model, x_raw_test, labels_test, 'Test')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
