{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import healpy as hp\n",
    "\n",
    "import matplotlib.pylab as pl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load some spherical data \n",
    "\n",
    "The data consists of ...\n",
    "\n",
    "The produced maps have been down-sampled from `NSIDE=1024` to `NSIDE=64` using the `ud_grade` function of the `healpy` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('data/maps_downsampled_64.npz')\n",
    "assert(len(data['class1'])==len(data['class2']))\n",
    "nclass = len(data['class1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us plot a map of each class. It is not simple to see a difference by eye."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmin = min(np.min(data['class1']), np.min(data['class2']))\n",
    "cmax = max(np.max(data['class1']), np.max(data['class2']))\n",
    "hp.mollview(data['class1'][0], title='class 1, Omega_matter=0.7', nest=True,  cmap='jet', min=cmin ,max=cmax)\n",
    "hp.mollview(data['class2'][0], title='class 2, Omega_matter=0.5', nest=True,  cmap='jet', min=cmin ,max=cmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, those maps have different power spectral densities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def psd(x):\n",
    "    '''Spherical Power Spectral Densities'''\n",
    "    hatx = hp.map2alm(hp.reorder(x, n2r=True))\n",
    "    p = hp.alm2cl(hatx)\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(psd(data['class1'][0]), label='class 1, Omega_matter=0.3, sample 0')\n",
    "plt.plot(psd(data['class1'][1]), label='class 2, Omega_matter=0.5, sample 0')\n",
    "plt.xscale('log')\n",
    "plt.legend()\n",
    "_ = pl.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When averaging over all the samples, and hence getting closer to the true PSD estimate, the statistical difference becomes obvious."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_psd_class1 = []\n",
    "sample_psd_class2 = []\n",
    "for i in range(nclass):\n",
    "    sample_psd_class1.append(psd(data['class1'][i]))\n",
    "    sample_psd_class2.append(psd(data['class2'][i]))\n",
    "sample_psd_class1 = np.vstack(sample_psd_class1)\n",
    "sample_psd_class2 = np.vstack(sample_psd_class2)\n",
    "\n",
    "psd_class1 = np.mean(sample_psd_class1, axis=0)\n",
    "psd_class2 = np.mean(sample_psd_class2, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(psd_class1, label='class 1, Omega_matter=0.3, mean')\n",
    "plt.plot(psd_class2, label='class 2, Omega_matter=0.5, mean')\n",
    "plt.xscale('log')\n",
    "plt.legend()\n",
    "_ = pl.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing data for the classifier\n",
    "Let us split the data into training and testing sets. The raw data is stored into `x_raw` and the power spectrum densities into `x_psd`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "# Put all the data in a single matrix\n",
    "x_raw = np.vstack((data['class1'],data['class2']))\n",
    "x_raw = x_raw / np.mean(x_raw*x_raw) # Apply some normalization (We do not want to affect the mean)\n",
    "x_psd = preprocessing.scale(np.vstack((sample_psd_class1,sample_psd_class2)))\n",
    "\n",
    "# Create the label vector\n",
    "labels = np.zeros([x_raw.shape[0]])\n",
    "labels[len(data['class2']):] = 1\n",
    "labels = labels.astype(np.int)\n",
    "\n",
    "# Random reordering\n",
    "p = np.random.permutation(len(labels))\n",
    "x_raw = x_raw[p,]\n",
    "x_psd = x_psd[p,]\n",
    "labels = labels[p]\n",
    "\n",
    "ntrain = 150\n",
    "ntest = len(x_raw) - ntrain\n",
    "x_raw_train = x_raw[:ntrain,]\n",
    "x_psd_train = x_psd[:ntrain,]\n",
    "labels_train = labels[:ntrain]\n",
    "x_raw_test = x_raw[ntrain:,]\n",
    "x_psd_test = x_psd[ntrain:,]\n",
    "labels_test = labels[ntrain:]\n",
    "\n",
    "print('Class 1 VS class 2 \\n  Training set: {} / {}\\n  Testing set: {} / {}'.format(\n",
    "    ntrain-sum(labels_train), sum(labels_train), ntest-sum(labels_test), sum(labels_test) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification using SVM\n",
    "Let us test classify our data using an SVM classifier\n",
    "\n",
    "An SVM based on the raw feature cannot discriminate the data because the dimensionality of the data is too large. However we observe the PSD features are linearly separable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "clf_raw = SVC(kernel='rbf')\n",
    "clf_raw.fit(x_raw_train, labels_train) \n",
    "\n",
    "print('Errors on training: {}%'.format(\n",
    "    sum(np.abs(clf_raw.predict(x_raw_train)-labels_train))/ntrain*100))\n",
    "print('Errors on testing: {}%'.format(\n",
    "    sum(np.abs(clf_raw.predict(x_raw_test)-labels_test))/ntest*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_psd = SVC(kernel='linear')\n",
    "clf_psd.fit(x_psd_train, labels_train) \n",
    "\n",
    "print('Errors on training: {}%'.format(\n",
    "    sum(np.abs(clf_psd.predict(x_psd_train)-labels_train))/ntrain*100))\n",
    "print('Errors on testing: {}%'.format(\n",
    "    sum(np.abs(clf_psd.predict(x_psd_test)-labels_test))/ntest*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification using a spherical CNN\n",
    "Let us now classify our data using a spherical convolutional neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scnn import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = 2 # number of class\n",
    "\n",
    "params = dict()\n",
    "params['dir_name']       = 'test'\n",
    "params['num_epochs']     = 5\n",
    "params['batch_size']     = 10\n",
    "params['eval_frequency'] = 10\n",
    "\n",
    "# Building blocks.\n",
    "params['brelu']          = 'b1relu' # Relu\n",
    "params['pool']           = 'apool1' # Average pooling\n",
    "\n",
    "# Architecture.\n",
    "params['nsides']         = [64, 32, 16] # Sizes of the laplacians are nsides * nisides * 12\n",
    "params['F']              = [5, 10, 10]  # Number of graph convolutional filters.\n",
    "params['K']              = [10, 10, 10]  # Polynomial orders.\n",
    "params['batch_norm']     = [True, True, True]  # Batch norm\n",
    "params['M']              = [100, C]  # Output dimensionality of fully connected layers.\n",
    "\n",
    "# Optimization.\n",
    "params['regularization'] = 1e-4\n",
    "params['dropout']        = 0.8\n",
    "params['learning_rate']  = 1e-3\n",
    "params['decay_rate']     = 0.95\n",
    "params['momentum']       = 0.9\n",
    "params['adam']           = True\n",
    "params['decay_steps']    = ntrain / params['batch_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.scnn(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy, loss, t_step = model.fit(x_raw_train, labels_train, x_raw_test, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Errors on training: {}%'.format(\n",
    "    sum(np.abs(model.predict(x_raw_train)-labels_train))/ntrain*100))\n",
    "print('Errors on testing: {}%'.format(\n",
    "    sum(np.abs(model.predict(x_raw_test)-labels_test))/ntest*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
