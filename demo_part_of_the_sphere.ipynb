{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import healpy as hp\n",
    "import matplotlib.pylab as pl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load some spherical data \n",
    "\n",
    "Let us load two maps with the same PSD and different high order statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1 = hp.read_map('data/same_psd/kappa_omega_m_0p3.fits')\n",
    "img2 = hp.read_map('data/same_psd/kappa_omega_m_0p26.fits')\n",
    "img1 = hp.reorder(img1,r2n=True)\n",
    "img2 = hp.reorder(img2,r2n=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Down sampling of the maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nside = 1024\n",
    "img1 = hp.ud_grade(img1,nside_out=Nside, order_in='NESTED')\n",
    "img2 = hp.ud_grade(img2,nside_out=Nside, order_in='NESTED')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us display the two maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmin = min(np.min(img1), np.min(img2))\n",
    "cmax = max(np.max(img1), np.max(img2))\n",
    "hp.mollview(img1, title='Map 1, omega_m=0.31, pk_norm=0.82, h=0.7', nest=True, min=cmin ,max=cmax)\n",
    "hp.mollview(img2, title='Map 2, omega_m=0.26, sigma_8=0.91, h=0.7', nest=True, min=cmin ,max=cmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us cut the sphere into 192 smaller subparts. We display 16 of them bellow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order = 4\n",
    "\n",
    "index = np.array(list(range(hp.nside2npix(order))))+2\n",
    "mask = np.zeros(index.shape,dtype=np.int)\n",
    "mask[:order**2] = 1\n",
    "index  *= mask\n",
    "hp.mollview(index, title='Some sphere subparts', nest=True)\n",
    "\n",
    "marker = np.zeros(hp.nside2npix(order))\n",
    "marker[0] = 1\n",
    "hp.mollview(marker, title='Selected indexes', nest=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hp_split(img, order, nest=True):\n",
    "    ''' This function split the data of different part of the sphere. \n",
    "        Return the splitted data and some possible index on the sphere\n",
    "    '''\n",
    "    npix = len(img)\n",
    "    nside = hp.npix2nside(npix)\n",
    "    if hp.nside2order(nside) < nside:\n",
    "        ValueError('Order not compatible with data')\n",
    "    if not nest:\n",
    "        NotImplementedError('Implement the change of coordidinate')\n",
    "    nsample = 12 * (order**2)\n",
    "    return img.reshape([nsample,npix//nsample]), np.array(list(range(npix//nsample)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dict()\n",
    "data['class1'], _ = hp_split(img1, order=4)\n",
    "data['class2'], index = hp_split(img2, order=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The data is of shape {}'.format(data['class1'].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us diplay one data sample on the entire sphere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# npix = hp.nside2npix(nside)\n",
    "# mask = np.ones([npix])>0\n",
    "# mask[index] = False\n",
    "# hp.ma()\n",
    "imgt = img1.copy()\n",
    "imgt[data['class1'].shape[1]:]=hp.UNSEEN\n",
    "imgt = hp.ma(imgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projected_map = hp.mollview(imgt,nest=True, return_projected_map = True, xsize = 1600 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(projected_map[380:520, 530:670])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing data for the classifier\n",
    "Let us split the data into training and testing sets. The raw data is stored into `x_raw` and the histograms into `x_trans`. As a transformation, we cannot use the power spectrum density. Hence we do an histogram of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trans(x,cmin,cmax):\n",
    "    if len(x.shape)>1:\n",
    "        d = []\n",
    "        for el in x:\n",
    "            y,_ = np.histogram(el, bins=100, range=[cmin,cmax])\n",
    "            d.append(y.astype(float))\n",
    "        return np.array(d)\n",
    "    else:\n",
    "        y,_ = np.histogram(x, bins=100, range=[cmin,cmax])\n",
    "        return y.astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "# Put all the data in a single matrix\n",
    "x_raw = np.vstack((data['class1'],data['class2']))\n",
    "x_raw = x_raw / np.mean(x_raw*x_raw) # Apply some normalization (We do not want to affect the mean)\n",
    "cmin = np.min(x_raw)\n",
    "cmax = np.max(x_raw)\n",
    "x_trans = trans(x_raw, cmin, cmax)\n",
    "x_trans = preprocessing.scale(x_trans)\n",
    "\n",
    "\n",
    "\n",
    "# Create the label vector\n",
    "labels = np.zeros([x_raw.shape[0]])\n",
    "labels[len(data['class1']):] = 1\n",
    "labels = labels.astype(np.int)\n",
    "\n",
    "# Random reordering\n",
    "p = np.random.permutation(len(labels))\n",
    "x_raw = x_raw[p,]\n",
    "x_trans = x_trans[p,]\n",
    "labels = labels[p]\n",
    "\n",
    "ntrain = 300\n",
    "ntest = len(x_raw) - ntrain\n",
    "x_raw_train = x_raw[:ntrain]\n",
    "x_trans_train = x_trans[:ntrain]\n",
    "labels_train = labels[:ntrain]\n",
    "x_raw_test = x_raw[ntrain:]\n",
    "x_trans_test = x_trans[ntrain:]\n",
    "labels_test = labels[ntrain:]\n",
    "\n",
    "print('Class 1 VS class 2 \\n  Training set: {} / {}\\n  Testing set: {} / {}'.format(\n",
    "    ntrain-sum(labels_train), sum(labels_train), ntest-sum(labels_test), sum(labels_test) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification using SVM\n",
    "Let us test classify our data using an SVM classifier\n",
    "\n",
    "While running an SVM classifier on the data will fail because of their dimensionality, we observe that we can correctly classify our dataset using the histogram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make with an histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "clf_raw = SVC(kernel='rbf')\n",
    "clf_raw.fit(x_raw_train, labels_train) \n",
    "\n",
    "print('Errors on training: {}%'.format(\n",
    "    sum(np.abs(clf_raw.predict(x_raw_train)-labels_train))/ntrain*100))\n",
    "print('Errors on testing: {}%'.format(\n",
    "    sum(np.abs(clf_raw.predict(x_raw_test)-labels_test))/ntest*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "clf_raw = SVC(kernel='rbf')\n",
    "clf_raw.fit(x_trans_train, labels_train) \n",
    "\n",
    "print('Errors on training: {}%'.format(\n",
    "    sum(np.abs(clf_raw.predict(x_trans_train)-labels_train))/ntrain*100))\n",
    "print('Errors on testing: {}%'.format(\n",
    "    sum(np.abs(clf_raw.predict(x_trans_test)-labels_test))/ntest*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the histogram features\n",
    "Let us first plot the mean and then each feature individually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot(np.mean(trans(data['class1'], cmin, cmax),axis=0))\n",
    "# plot(np.mean(trans(data['class2'], cmin, cmax),axis=0))\n",
    "x_trans = trans(x_raw, cmin, cmax)\n",
    "fig = plt.figure()\n",
    "plot(np.mean(x_trans[labels==0],axis=0))\n",
    "plot(np.mean(x_trans[labels==1],axis=0))\n",
    "title('Mean of the classes')\n",
    "fig = plt.figure()\n",
    "plot(x_trans[labels==0].T, 'b')\n",
    "plot(x_trans[labels==1].T, 'r')\n",
    "_, title('Individual samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification using a spherical CNN\n",
    "Let us now classify our data using a spherical convolutional neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scnn import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsides = [Nside, Nside, Nside//2, min(Nside//8, 128)]\n",
    "# nsides = [2048, 1024, 256, 64]\n",
    "# nsides = [128, 32, 16]\n",
    "\n",
    "indexes = []\n",
    "nsample = 12 * (order**2)\n",
    "for nside in nsides:\n",
    "    indexes.append(np.array(list(range(hp.nside2npix(nside)//nsample))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = 2 # number of class\n",
    "\n",
    "params = dict()\n",
    "params['dir_name']       = 'test'\n",
    "params['num_epochs']     = 10\n",
    "params['batch_size']     = 20\n",
    "params['eval_frequency'] = 10\n",
    "\n",
    "# Building blocks.\n",
    "params['brelu']          = 'b1lrelu' # Relu \n",
    "params['pool']           = 'apool1' # Average pooling\n",
    "\n",
    "# Architecture.\n",
    "params['nsides']         = nsides # Sizes of the laplacians are nsides * nisides * 12\n",
    "params['indexes']        = indexes # Sizes of the laplacians are nsides * nisides * 12\n",
    "params['F']              = [5, 20, 80, 10]  # Number of graph convolutional filters.\n",
    "params['K']              = [10, 10, 10, 10]  # Polynomial orders.\n",
    "params['batch_norm']     = [True, True, True, True]  # Batch norm\n",
    "params['M']              = [100, C]  # Output dimensionality of fully connected layers.\n",
    "\n",
    "# Optimization.\n",
    "params['regularization'] = 2e-4\n",
    "params['dropout']        = 0.8\n",
    "params['learning_rate']  = 10e-4\n",
    "params['decay_rate']     = 0.95\n",
    "params['momentum']       = 0.9\n",
    "params['adam']           = True\n",
    "params['decay_steps']    = ntrain / params['batch_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.scnn(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy, loss, t_step = model.fit(x_raw_train, labels_train, x_raw_test, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Errors on training: {}%'.format(\n",
    "    sum(np.abs(model.predict(x_raw_train)-labels_train))/ntrain*100))\n",
    "print('Errors on testing: {}%'.format(\n",
    "    sum(np.abs(model.predict(x_raw_test)-labels_test))/ntest*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Comments about the results\n",
    "Without subsampling\n",
    "I train the spherical CNN a few minutes on CPU and I obtain 96% validation accuracy. \n",
    "\n",
    "SVM is consistenly failling with the raw data but succeed with the histograms.\n",
    "\n",
    "Conclusion: the spherical CNN is able to discriminate over data with the same mean and same PSD using only 192th of the sphere.\n",
    "\n",
    "Effect of subsampling\n",
    " - N=512, errors on training/testing: 11.66%, 78.57% => complete fail\n",
    " - N=1024, errors on training/testing: 0%, 0-3% => partial success\n",
    " - N=2048, errors on training/testing: 0%, 3% => partial success\n",
    "\n",
    "Maybe this is also due to the fact that the training/validation sets are not the same for each run.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some other plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scnn import utils\n",
    "# nside_v = 32\n",
    "# nsample = 12 * (order**2)\n",
    "# ind = np.array(list(range(hp.nside2npix(nside_v)//nsample)))\n",
    "# G = utils.healpix_graph(nside=nside_v, nest=True, indexes=ind)\n",
    "\n",
    "# G.plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
